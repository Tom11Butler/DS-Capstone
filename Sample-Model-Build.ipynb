{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkify Dev Build Workspace\n",
    "\n",
    "This workspace contains a tiny subset (128MB) of the full dataset available (12GB). It will be used for EDA to garner insights on what features will be useful to look into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import IntegerType, TimestampType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "from pyspark.sql.functions import desc, asc, sum as Fsum\n",
    "from pyspark.sql.functions import month, dayofmonth, dayofweek, hour\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler, Normalizer, StandardScaler\n",
    "from pyspark.ml.feature import StringIndexer, CountVectorizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Our first Python Spark SQL example\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.driver.host', '192.168.0.12'),\n",
       " ('spark.app.name', 'Our first Python Spark SQL example'),\n",
       " ('spark.driver.port', '58439'),\n",
       " ('spark.app.id', 'local-1593636902710'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.12:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Our first Python Spark SQL example</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fbc2256da10>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the data ...\n",
      "Data loaded!\n",
      "\n",
      "Cleaning the data ...\n",
      "Data cleaned!\n",
      "\n",
      "Extracting useable features from datetime timestamp ...\n",
      "Features extracted and added to table!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Loading the data ...')\n",
    "path = \"mini_sparkify_event_data.json\"\n",
    "event_log = spark.read.json(path)\n",
    "print('Data loaded!\\n')\n",
    "\n",
    "print('Cleaning the data ...')\n",
    "\n",
    "# removes the empty user ID\n",
    "event_log = event_log.filter(event_log.userId != \"\")\n",
    "\n",
    "# Flags if a user cancelled at any point\n",
    "flag_cancel_event = udf(lambda x:\n",
    "                        1 if x == 'Cancellation Confirmation' else 0\n",
    "                        , IntegerType())\n",
    "event_log = event_log.withColumn('cancelled'\n",
    "                                 , flag_cancel_event(event_log.page))\n",
    "\n",
    "# all rows after cancel event set churned to 1 else 0\n",
    "windowval = Window.partitionBy('userId')\\\n",
    "                    .orderBy(desc('ts'))\\\n",
    "                    .rangeBetween(Window.unboundedPreceding, 0)\n",
    "event_log = event_log.withColumn('churned', Fsum('cancelled').over(windowval))\n",
    "print('Data cleaned!\\n')\n",
    "\n",
    "\n",
    "print('Extracting useable features from datetime timestamp ...')\n",
    "get_timestamp = udf(lambda x:\n",
    "                    datetime.datetime.fromtimestamp(int(int(x)/1000))\n",
    "                    , TimestampType())\n",
    "event_log = event_log.withColumn('ts', get_timestamp(event_log.ts)) \\\n",
    "                    .withColumn('hour', hour('ts')) \\\n",
    "                    .withColumn('day', dayofmonth('ts')) \\\n",
    "                    .withColumn('month', month('ts')) \\\n",
    "                    .withColumn('weekday', dayofweek('ts'))\n",
    "print('Features extracted and added to table!\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_daily_use = event_log.groupby(['userId','month','day']).count() \\\n",
    "                        .sort('userId','month','day') \\\n",
    "                        .groupby('userId').avg('count') \\\n",
    "                        .withColumnRenamed('avg(count)','avg_daily_songplays')\n",
    "avg_daily_use.createOrReplaceTempView('avg_daily_use_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_log.createOrReplaceTempView('event_log_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+-------------+---------------+--------------+------------+-----------+------------+------------+--------+-------------------+-------+\n",
      "|userId|num_songplays|num_thumbs_up|num_thumbs_down|num_plylst_add|num_frnd_add|num_sav_set|num_ad_rolls|num_sessions|had_paid|avg_daily_songplays|churned|\n",
      "+------+-------------+-------------+---------------+--------------+------------+-----------+------------+------------+--------+-------------------+-------+\n",
      "|100010|          275|           17|              5|             7|           4|          0|          52|           7|       0|  54.42857142857143|      0|\n",
      "|200002|          387|           21|              6|             8|           4|          0|           7|           6|       1|  67.71428571428571|      0|\n",
      "|   125|            8|            0|              0|             0|           0|          0|           1|           1|       0|               11.0|      1|\n",
      "|   124|         4079|          171|             41|           118|          74|          6|           4|          29|       1| 141.91176470588235|      0|\n",
      "|    51|         2111|          100|             21|            52|          28|          1|           0|          10|       1|              176.0|      1|\n",
      "+------+-------------+-------------+---------------+--------------+------------+-----------+------------+------------+--------+-------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "build_user_table = spark.sql('''\n",
    "    SELECT\n",
    "        distinct e.userId\n",
    "        , sum(CASE WHEN page=\"NextSong\" THEN 1 ELSE 0 END) as num_songplays\n",
    "        , sum(CASE WHEN page=\"Thumbs Up\" THEN 1 ELSE 0 END) as num_thumbs_up\n",
    "        , sum(CASE WHEN page=\"Thumbs Down\" THEN 1 ELSE 0 END) as num_thumbs_down\n",
    "        , sum(CASE WHEN page=\"Add to Playlist\" THEN 1 ELSE 0 END) as num_plylst_add\n",
    "        , sum(CASE WHEN page=\"Add Friend\" THEN 1 ELSE 0 END) as num_frnd_add\n",
    "        , sum(CASE WHEN page=\"Save Settings\" THEN 1 ELSE 0 END) as num_sav_set\n",
    "        , sum(CASE WHEN page=\"Roll Advert\" THEN 1 ELSE 0 END) as num_ad_rolls\n",
    "        , count(distinct sessionId) as num_sessions\n",
    "        , max(CASE WHEN level=\"paid\" THEN 1 else 0 END) as had_paid\n",
    "        , max(adu.avg_daily_songplays) as avg_daily_songplays\n",
    "        , max(churned) as churned\n",
    "    FROM\n",
    "        event_log_table e\n",
    "    LEFT JOIN avg_daily_use_table adu\n",
    "        ON e.userId = adu.userId\n",
    "    GROUP BY\n",
    "        e.userId\n",
    "''')\n",
    "build_user_table.show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=['num_songplays','num_thumbs_up',\n",
    "                                       'num_thumbs_down','num_plylst_add',\n",
    "                                       'num_frnd_add','num_sav_set',\n",
    "                                       'num_ad_rolls','num_sessions',\n",
    "                                       'avg_daily_songplays']\n",
    "                           , outputCol='NumFeatures')\n",
    "build_user_table = assembler.transform(build_user_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = Normalizer(inputCol='NumFeatures', outputCol='ScaledNumFeatures')\n",
    "build_user_table = scaler.transform(build_user_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label=0, features=DenseVector([0.962, 0.0595, 0.0175, 0.0245, 0.014, 0.0, 0.1819, 0.0245, 0.1904])),\n",
       " Row(label=0, features=DenseVector([0.983, 0.0533, 0.0152, 0.0203, 0.0102, 0.0, 0.0178, 0.0152, 0.172])),\n",
       " Row(label=1, features=SparseVector(9, {0: 0.585, 6: 0.0731, 7: 0.0731, 8: 0.8044}))]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = build_user_table.select(col('churned').alias('label')\n",
    "                               , col('ScaledNumFeatures').alias('features'))\n",
    "data.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|    0|[0.96198859849549...|\n",
      "|    0|[0.98299295498964...|\n",
      "|    1|(9,[0,6,7,8],[0.5...|\n",
      "|    0|[0.99786259670431...|\n",
      "|    1|[0.99498824871416...|\n",
      "+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = data.randomSplit([0.8,0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(maxIter=10, regParam=0.0)\n",
    "dt = DecisionTreeClassifier(seed=7)\n",
    "rf = RandomForestClassifier(seed=7)\n",
    "SVM = LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol='prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression is training...\n",
      "LogisticRegression is predicting...\n",
      "LogisticRegression is evaluating...\n",
      "Test F1-score: 0.6223938223938223\n",
      "\n",
      "DecisionTreeClassifier is training...\n",
      "DecisionTreeClassifier is predicting...\n",
      "DecisionTreeClassifier is evaluating...\n",
      "Test F1-score: 0.6425966636492954\n",
      "\n",
      "RandomForestClassifier is training...\n",
      "RandomForestClassifier is predicting...\n",
      "RandomForestClassifier is evaluating...\n",
      "Test F1-score: 0.6023166023166023\n",
      "\n",
      "LinearSVC is training...\n",
      "LinearSVC is predicting...\n",
      "LinearSVC is evaluating...\n",
      "Test F1-score: 0.6157094594594593\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_results = {}\n",
    "for model in [lr, dt, rf, SVM]:\n",
    "    model_results = {}\n",
    "    # get the classifier name\n",
    "    model_name = model.__class__.__name__\n",
    "    \n",
    "    \n",
    "    # fit the dataset\n",
    "    print(f'{model_name} is training...')\n",
    "    start = time.time() \n",
    "    model = model.fit(train)\n",
    "    end = time.time() \n",
    "    model_results['train_time'] = round(end-start,6)\n",
    "    \n",
    "    # predict\n",
    "    print(f'{model_name} is predicting...')\n",
    "    start = time.time() \n",
    "    pred_test = model.transform(test)\n",
    "    end = time.time()\n",
    "    model_results['pred_time'] = round(end-start,6)\n",
    "    \n",
    "    #metrics\n",
    "    print(f'{model_name} is evaluating...')    \n",
    "    model_results['f1_test'] = evaluator.evaluate(pred_test.select('label'\n",
    "                                                                ,'prediction')\n",
    "                                                  ,{evaluator.metricName: 'f1'})\n",
    "    print('Test F1-score: {}\\n'.format(model_results['f1_test']))\n",
    "    all_results[model_name] = model_results\n",
    "    \n",
    "all_results_df = pd.DataFrame(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LogisticRegression</th>\n",
       "      <th>DecisionTreeClassifier</th>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <th>LinearSVC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train_time</th>\n",
       "      <td>150.231972</td>\n",
       "      <td>288.026072</td>\n",
       "      <td>237.628994</td>\n",
       "      <td>422.983274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pred_time</th>\n",
       "      <td>0.056735</td>\n",
       "      <td>0.062789</td>\n",
       "      <td>0.044093</td>\n",
       "      <td>0.054641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1_test</th>\n",
       "      <td>0.622394</td>\n",
       "      <td>0.642597</td>\n",
       "      <td>0.602317</td>\n",
       "      <td>0.615709</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            LogisticRegression  DecisionTreeClassifier  \\\n",
       "train_time          150.231972              288.026072   \n",
       "pred_time             0.056735                0.062789   \n",
       "f1_test               0.622394                0.642597   \n",
       "\n",
       "            RandomForestClassifier   LinearSVC  \n",
       "train_time              237.628994  422.983274  \n",
       "pred_time                 0.044093    0.054641  \n",
       "f1_test                   0.602317    0.615709  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thoughts\n",
    "\n",
    "- The LinearSVC is the most expensive to run and ranks 3rd out of 4 models for F1 score and so shall be discarded.\n",
    "- Logistic Regression and the Decision Tree shall be taken forward for tuning given they performed the best and are not too expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning the model\n",
    "\n",
    "The model will be tuned here with **CrossValidator** rather than **TrainValidationSplit** as the data set is smaller and it is a well-established method for choosing parameters and is more statistically sound.\n",
    "\n",
    "## Tune the Linear Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = ParamGridBuilder(). \\\n",
    "            addGrid(lr.elasticNetParam,[0.1,0.5,1]). \\\n",
    "            addGrid(lr.regParam,[0.01,0.05,0.1]). \\\n",
    "            build()\n",
    "\n",
    "crossval = CrossValidator(estimator=lr,\n",
    "                 estimatorParamMaps=paramGrid,\n",
    "                 evaluator=MulticlassClassificationEvaluator(metricName='f1'),\n",
    "                 numFolds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model tuning complete - took 1271.9184019565582s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "cv_lr = crossval.fit(train)\n",
    "end = time.time()\n",
    "print('Model tuning complete - took {}s'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7136412830477706,\n",
       " 0.7087677756686499,\n",
       " 0.7113642778900438,\n",
       " 0.7136412830477706,\n",
       " 0.7049906457351616,\n",
       " 0.6848707987840734,\n",
       " 0.7136412830477706,\n",
       " 0.6807680998501884,\n",
       " 0.6795832082043632]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_lr.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegressionModel: uid=LogisticRegression_a7affad617e6, numClasses=2, numFeatures=9"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_lr.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_lr_results = cv_lr.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6756756756756757\n",
      "F-1 Score:0.6223938223938223\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: {}'.format(evaluator.evaluate(cv_lr_results.select('label','prediction'), {evaluator.metricName: \"accuracy\"})))\n",
    "print('F-1 Score:{}'.format(evaluator.evaluate(cv_lr_results.select('label','prediction'), {evaluator.metricName: \"f1\"})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune the Decision Tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = ParamGridBuilder(). \\\n",
    "            addGrid(dt.impurity, ['entropy','gini']). \\\n",
    "            addGrid(dt.maxDepth, [x for x in range(5,25,5)]). \\\n",
    "            build()\n",
    "\n",
    "crossval = CrossValidator(estimator=lr,\n",
    "                 estimatorParamMaps=paramGrid,\n",
    "                 evaluator=MulticlassClassificationEvaluator(metricName='f1'),\n",
    "                 numFolds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model tuning complete - took 1182.3541371822357s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "cv_dt = crossval.fit(train)\n",
    "end = time.time()\n",
    "print('Model tuning complete - took {}s'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.712487944706165,\n",
       " 0.712487944706165,\n",
       " 0.712487944706165,\n",
       " 0.712487944706165,\n",
       " 0.712487944706165,\n",
       " 0.712487944706165,\n",
       " 0.712487944706165,\n",
       " 0.712487944706165]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_dt.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_dt_results = cv_dt.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6756756756756757\n",
      "F-1 Score:0.6223938223938223\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: {}'.format(evaluator\\\n",
    "                            .evaluate(cv_dt_results.select('label'\n",
    "                                                           ,'prediction')\n",
    "                                      , {evaluator.metricName: \"accuracy\"})))\n",
    "print('F-1 Score:{}'.format(evaluator\\\n",
    "                            .evaluate(cv_dt_results.select('label'\n",
    "                                                           ,'prediction')\n",
    "                                      , {evaluator.metricName: \"f1\"})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "The **Logistic Regression** model looks to perform best under the tuning with an F1 score of $0.675$.\n",
    "\n",
    "Both of the logistic regression and decision tree classifier models will be taken forward and explored on the AWS EMR cluster to model the full 12GB dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
